{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import dotenv\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "import time\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from chromadb.config import Settings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_APIKEY\"),  \n",
    "    api_version=os.getenv('OPEN_API_VERSION'),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(\"./philosophy.csv\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name = \"text-embedding-01\"\n",
    "\n",
    "# Define chunk size and pause duration\n",
    "chunk_size = 150\n",
    "pause_duration = 1  # pause for 5 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = pages[0:chunk_size]\n",
    "embeddings = AzureOpenAIEmbeddings(azure_deployment=deployment_name)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunk,\n",
    "    embedding=embeddings,\n",
    "    client_settings=Settings(),\n",
    "    persist_directory=\"philosophy_head.db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents 150 to 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w5/9wvkhys54sd_f1x0tgfg9rkm0000gn/T/ipykernel_74203/3273429229.py:5: RuntimeWarning: coroutine 'VectorStore.aadd_documents' was never awaited\n",
      "  vectorstore.aadd_documents(\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents 300 to 450\n",
      "Processing documents 450 to 600\n",
      "Processing documents 600 to 750\n",
      "Processing documents 750 to 900\n",
      "Processing documents 900 to 1050\n",
      "Processing documents 1050 to 1200\n",
      "Processing documents 1200 to 1350\n",
      "Processing documents 1350 to 1500\n",
      "Processing documents 1500 to 1650\n",
      "Processing documents 1650 to 1800\n",
      "Processing documents 1800 to 1950\n",
      "Processing documents 1950 to 2100\n",
      "Processing documents 2100 to 2250\n",
      "Processing documents 2250 to 2400\n",
      "Processing documents 2400 to 2550\n",
      "Processing documents 2550 to 2700\n",
      "Processing documents 2700 to 2850\n",
      "Processing documents 2850 to 3000\n",
      "Processing documents 3000 to 3150\n",
      "Processing documents 3150 to 3300\n",
      "Processing documents 3300 to 3450\n",
      "Processing documents 3450 to 3600\n",
      "Processing documents 3600 to 3750\n",
      "Processing documents 3750 to 3900\n",
      "Processing documents 3900 to 4050\n",
      "Processing documents 4050 to 4200\n",
      "Processing documents 4200 to 4350\n",
      "Processing documents 4350 to 4500\n",
      "Processing documents 4500 to 4650\n",
      "Processing documents 4650 to 4800\n",
      "Processing documents 4800 to 4950\n",
      "Processing documents 4950 to 5100\n",
      "Processing documents 5100 to 5250\n",
      "Processing documents 5250 to 5400\n",
      "Processing documents 5400 to 5550\n",
      "Processing documents 5550 to 5700\n",
      "Processing documents 5700 to 5850\n",
      "Processing documents 5850 to 6000\n",
      "Processing documents 6000 to 6150\n",
      "Processing documents 6150 to 6300\n",
      "Processing documents 6300 to 6450\n",
      "Processing documents 6450 to 6600\n",
      "Processing documents 6600 to 6750\n",
      "Processing documents 6750 to 6900\n",
      "Processing documents 6900 to 7050\n",
      "Processing documents 7050 to 7200\n",
      "Processing documents 7200 to 7350\n",
      "Processing documents 7350 to 7500\n",
      "Processing documents 7500 to 7650\n",
      "Processing documents 7650 to 7800\n",
      "Processing documents 7800 to 7950\n",
      "Processing documents 7950 to 8100\n",
      "Processing documents 8100 to 8250\n",
      "Processing documents 8250 to 8400\n",
      "Processing documents 8400 to 8550\n",
      "Processing documents 8550 to 8700\n",
      "Processing documents 8700 to 8850\n",
      "Processing documents 8850 to 9000\n",
      "Processing documents 9000 to 9150\n",
      "Processing documents 9150 to 9300\n",
      "Processing documents 9300 to 9450\n",
      "Processing documents 9450 to 9600\n",
      "Processing documents 9600 to 9750\n",
      "Processing documents 9750 to 9900\n"
     ]
    }
   ],
   "source": [
    "# Create Chroma vectorstore from documents in chunks\n",
    "for i in range(chunk_size, len(pages), chunk_size):\n",
    "    print(f\"Processing documents {i} to {i+chunk_size}\")\n",
    "    chunk = pages[i:i+chunk_size]\n",
    "    vectorstore.aadd_documents(\n",
    "        documents=chunk,\n",
    "        embedding=embeddings,\n",
    "        client_settings=Settings()\n",
    "    )\n",
    "    time.sleep(pause_duration)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.2\n",
    "deployment_name = \"gpt-model-01\"\n",
    "llm = AzureChatOpenAI(temperature=temperature, azure_deployment=deployment_name)\n",
    "\n",
    "def format_docs(docs):\n",
    "    doc = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "condense_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "condense_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "condense_q_chain = condense_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the definition of \"large\" in the context of a language model?'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.messages import AIMessage, HumanMessage\n",
    "\n",
    "condense_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"What is meant by large\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks who is an expert in philosophy. Answer the question only if it is related to philosophy and otherwise say that it isn't part of your domain. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def condense_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return condense_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=condense_question | retriever | format_docs)\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"What is meant by large\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def tts(text):\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('SPEECH_KEY'), region=os.environ.get('SPEECH_REGION'))\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "    # The language of the voice that speaks.\n",
    "    speech_config.speech_synthesis_voice_name='en-US-JennyNeural'\n",
    "\n",
    "    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # Get text from the console and synthesize to the default speaker.\n",
    "    print(\"Enter some text that you want to speak >\")\n",
    "    text = input()\n",
    "\n",
    "    speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()\n",
    "\n",
    "    if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(\"Speech synthesized for text [{}]\".format(text))\n",
    "    elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_synthesis_result.cancellation_details\n",
    "        print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            if cancellation_details.error_details:\n",
    "                print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "                print(\"Did you set the speech resource key and region values?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_response(question, history):\n",
    "    \"\"\"\n",
    "      message: new message from the user\n",
    "      history: history of messages from user\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    for e in history:\n",
    "      if e[0] == None:\n",
    "        messages.append(AIMessage(content=\"\"))\n",
    "      else:\n",
    "        messages.append(HumanMessage(content=e[0]))\n",
    "      messages.append(AIMessage(content=e[1]))\n",
    "\n",
    "    messages=messages[:10]\n",
    "    answer = rag_chain.invoke(\n",
    "        {\n",
    "          \"chat_history\": messages,\n",
    "          \"question\": question,\n",
    "        }\n",
    "    ).content\n",
    "\n",
    "    tts(answer)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def recognize_from_microphone():\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('SPEECH_KEY'), region=os.environ.get('SPEECH_REGION'))\n",
    "    speech_config.speech_recognition_language=\"en-US\"\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        return speech_recognition_result.text\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        return f'No speech could be recognized: {speech_recognition_result.no_match_details}'\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "            return f'Speech Recognition canceled: {cancellation_details.error_details}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2853146152.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[82], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    if speech_input:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# def process_input(text_input, speech_input):\n",
    "    if speech_input:\n",
    "        text_input = recognize_from_microphone()\n",
    "    response = produce_response(text_input)\n",
    "    return response\n",
    "\n",
    "# demo = gr.Interface(\n",
    "#     process_input,\n",
    "#     ['textbox', 'textbox'],\n",
    "#     'textbox',\n",
    "#     title=\"OpenAI Chatbot Example\",\n",
    "#     description=\"A chatbot example for QCRI Generative AI Hackathon 2023\",\n",
    "# )\n",
    "\n",
    "# demo.launch()\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "  produc  response,\n",
    "#   titl  \"OpenAI Chatb# ot Example\",\n",
    "  de  ription=\"A chatbot example for#  QCRI Generative AI Hackathon 2023\",\n",
    "    atb# ot=gr.Chatbot(value=[(None, \"Welcome\")])\n",
    "  )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
